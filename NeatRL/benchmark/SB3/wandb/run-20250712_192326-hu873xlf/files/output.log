Using cuda device
/home/yuvrajsingh/anaconda3/envs/mt/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.
  warnings.warn(
ActorCriticPolicy(
  (features_extractor): FlattenExtractor(
    (flatten): Flatten(start_dim=1, end_dim=-1)
  )
  (pi_features_extractor): FlattenExtractor(
    (flatten): Flatten(start_dim=1, end_dim=-1)
  )
  (vf_features_extractor): FlattenExtractor(
    (flatten): Flatten(start_dim=1, end_dim=-1)
  )
  (mlp_extractor): MlpExtractor(
    (policy_net): Sequential(
      (0): Linear(in_features=17, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=64, bias=True)
      (3): Tanh()
    )
    (value_net): Sequential(
      (0): Linear(in_features=17, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=64, bias=True)
      (3): Tanh()
    )
  )
  (action_net): Linear(in_features=64, out_features=6, bias=True)
  (value_net): Linear(in_features=64, out_features=1, bias=True)
)
[2K---------------------------------â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m15,856/10,000,000 [0m [ [33m0:00:02[0m < [36m0:24:04[0m , [31m6,918 it/s[0m ]
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -401     |
| time/              |          |
|    fps             | 6162     |
|    iterations      | 1        |
|    time_elapsed    | 2        |
|    total_timesteps | 16384    |
---------------------------------
[2Kwandb: WARNING Step cannot be set when using tensorboard syncing. Please use `run.define_metric(...)` to define a custom metric to log your step values.m15,856/10,000,000 [0m [ [33m0:00:04[0m < [36m0:24:04[0m , [31m6,918 it/s[0m ]
[2K-----------------------------------------â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m32,080/10,000,000 [0m [ [33m0:00:06[0m < [36m0:32:58[0m , [31m5,041 it/s[0m ]
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -374        |
| time/                   |             |
|    fps                  | 4857        |
|    iterations           | 2           |
|    time_elapsed         | 6           |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.007527722 |
|    clip_fraction        | 0.0713      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.52       |
|    explained_variance   | -0.769      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0222      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00741    |
|    std                  | 1           |
|    value_loss           | 0.303       |
-----------------------------------------
[2K-----------------------------------------8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m48,832/10,000,000 [0m [ [33m0:00:10[0m < [36m0:34:19[0m , [31m4,834 it/s[0m ]
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -365        |
| time/                   |             |
|    fps                  | 4711        |
|    iterations           | 3           |
|    time_elapsed         | 10          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.005986628 |
|    clip_fraction        | 0.0531      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.54       |
|    explained_variance   | 0.0558      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0418     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00661    |
|    std                  | 1           |
|    value_loss           | 0.104       |
-----------------------------------------
[2K-----------------------------------------8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m64,656/10,000,000 [0m [ [33m0:00:14[0m < [36m0:35:13[0m , [31m4,703 it/s[0m ]
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -350        |
| time/                   |             |
|    fps                  | 4623        |
|    iterations           | 4           |
|    time_elapsed         | 14          |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.006442965 |
|    clip_fraction        | 0.0526      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.53       |
|    explained_variance   | 0.212       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0521     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00709    |
|    std                  | 1           |
|    value_loss           | 0.103       |
-----------------------------------------
[2K-----------------------------------------8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m81,216/10,000,000 [0m [ [33m0:00:18[0m < [36m0:36:06[0m , [31m4,580 it/s[0m ]
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | -341        |
| time/                   |             |
|    fps                  | 4519        |
|    iterations           | 5           |
|    time_elapsed         | 18          |
|    total_timesteps      | 81920       |
| train/                  |             |
|    approx_kl            | 0.006500936 |
|    clip_fraction        | 0.0637      |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.53       |
|    explained_variance   | 0.278       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0493     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00788    |
|    std                  | 1           |
|    value_loss           | 0.0735      |
-----------------------------------------
[2KTraceback (most recent call last):[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m81,888/10,000,000 [0m [ [33m0:00:19[0m < [36m0:36:01[0m , [31m4,592 it/s[0m ]
[2K  File "/mnt/c/Users/yuvra/OneDrive/Desktop/Work/pytorch/RL/PPO/MuJoCo/benchmark/SB3/half-cheetah-sb3.py", line 196, in <module>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m81,888/10,000,000 [0m [ [33m0:00:19[0m < [36m0:36:01[0m , [31m4,592 it/s[0m ]
    model.learn(
[2K  File "/home/yuvrajsingh/anaconda3/envs/mt/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py", line 311, in learnâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m81,888/10,000,000 [0m [ [33m0:00:19[0m < [36m0:36:01[0m , [31m4,592 it/s[0m ]
    return super().learn(
           ^^^^^^^^^^^^^^
[2K  File "/home/yuvrajsingh/anaconda3/envs/mt/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 337, in learnâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m81,888/10,000,000 [0m [ [33m0:00:19[0m < [36m0:36:01[0m , [31m4,592 it/s[0m ]
    self.train()
[2K  File "/home/yuvrajsingh/anaconda3/envs/mt/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py", line 278, in trainâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m81,888/10,000,000 [0m [ [33m0:00:19[0m < [36m0:36:01[0m , [31m4,592 it/s[0m ]
    self.policy.optimizer.step()
[2K  File "/home/yuvrajsingh/anaconda3/envs/mt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 493, in wrapperâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m81,888/10,000,000 [0m [ [33m0:00:19[0m < [36m0:36:01[0m , [31m4,592 it/s[0m ]
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
[2K  File "/home/yuvrajsingh/anaconda3/envs/mt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_gradâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m81,888/10,000,000 [0m [ [33m0:00:19[0m < [36m0:36:01[0m , [31m4,592 it/s[0m ]
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2K  File "/home/yuvrajsingh/anaconda3/envs/mt/lib/python3.11/site-packages/torch/optim/adam.py", line 244, in stepâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m81,888/10,000,000 [0m [ [33m0:00:19[0m < [36m0:36:01[0m , [31m4,592 it/s[0m ]
    adam(
[2K  File "/home/yuvrajsingh/anaconda3/envs/mt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallbackâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m81,888/10,000,000 [0m [ [33m0:00:19[0m < [36m0:36:01[0m , [31m4,592 it/s[0m ]
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
[2K  File "/home/yuvrajsingh/anaconda3/envs/mt/lib/python3.11/site-packages/torch/optim/adam.py", line 876, in adamâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m81,888/10,000,000 [0m [ [33m0:00:19[0m < [36m0:36:01[0m , [31m4,592 it/s[0m ]
    func(
[2K  File "/home/yuvrajsingh/anaconda3/envs/mt/lib/python3.11/site-packages/torch/optim/adam.py", line 707, in _multi_tensor_adamâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m81,888/10,000,000 [0m [ [33m0:00:19[0m < [36m0:36:01[0m , [31m4,592 it/s[0m ]
    torch._foreach_addcdiv_(
[2KKeyboardInterrupt;249;38;114mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m81,888/10,000,000 [0m [ [33m0:00:19[0m < [36m0:36:01[0m , [31m4,592 it/s[0m ]
[2K[35m   1%[0m [38;2;249;38;114mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m81,888/10,000,000 [0m [ [33m0:00:20[0m < [36m0:36:01[0m , [31m4,592 it/s[0m ]
