{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n\nimport os\nimport random\nimport time\nimport gymnasium as gym\nfrom tqdm import tqdm\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nimport wandb\nimport cv2\nimport imageio\nimport ale_py\n\nfrom stable_baselines3.common.atari_wrappers import (\n    ClipRewardEnv,\n    EpisodicLifeEnv,\n    FireResetEnv,\n    MaxAndSkipEnv,\n    NoopResetEnv,\n)\n\ngym.register_envs(ale_py)\n# from vizdoom import gymnasium_wrapper # Ensure ViZDoom is registered\n\n# ===== CONFIGURATION =====\nclass Config:\n    # Experiment settings\n    exp_name = \"PPO-Vectorized-Atari\"\n    seed = 42\n    env_id = \"PongNoFrameskip-v4\"\n    total_timesteps = 10_000_000  # Standard metric for vectorized training\n\n    # PPO & Agent settings\n    lr = 2.5e-4\n    gamma = 0.99\n    num_envs = 8  # Number of parallel environments\n    max_steps = 128  # Steps per rollout per environment (aka num_steps)\n    num_minibatches = 4\n    PPO_EPOCHS = 4\n    clip_value = 0.1 \n    clip_coeff = 0.1  # Value clipping coefficient\n    ENTROPY_COEFF = 0.01\n    \n    VALUE_COEFF = 0.5\n    \n    # Logging & Saving\n    capture_video = True\n    use_wandb = True\n    wandb_project = \"cleanRL\"\n    \n    GAE = 0.95  # Generalized Advantage Estimation\n    anneal_lr = True  # Whether to linearly decay the learning rate\n    max_grad_norm = 0.5  # Gradient clipping value\n    \n    \n    # Derived values\n    @property\n    def batch_size(self):\n        return self.num_envs * self.max_steps\n\n    @property\n    def minibatch_size(self):\n        return self.batch_size // self.num_minibatches\n\n# --- Preprocessing ---\nTARGET_HEIGHT = 64\nTARGET_WIDTH = 64\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# --- Networks ---\ndef layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n    torch.nn.init.orthogonal_(layer.weight, std)\n    torch.nn.init.constant_(layer.bias, bias_const)\n    return layer\n\nclass Agent(nn.Module):\n    def __init__(self, action_space):\n        super(Agent, self).__init__()\n        # Shared CNN feature extractor\n        self.network = nn.Sequential(\n            layer_init(nn.Conv2d(4, 32, kernel_size=8, stride=4)),\n            nn.ReLU(),\n            layer_init(nn.Conv2d(32, 64, kernel_size=4, stride=2)),\n            nn.ReLU(),\n            layer_init(nn.Conv2d(64, 64, kernel_size=3, stride=1)),\n            nn.ReLU(),\n            nn.Flatten(),\n            layer_init(nn.Linear(64 * 7 * 7, 512)), # Adjusted for 64x64 input\n            nn.ReLU(),\n        )\n        # Actor head\n        self.actor = layer_init(nn.Linear(512, action_space), std=0.01)\n        # Critic head\n        self.critic = layer_init(nn.Linear(512, 1), std=1.0)\n\n    def get_features(self, x):\n        return self.network(x)\n\n    def get_value(self, x):\n        return self.critic(self.get_features(x))\n\n    def get_action(self, x, action=None, deterministic=False):\n        features = self.get_features(x)\n        logits = self.actor(features)\n        probs = torch.softmax(logits, dim=-1)\n        dist = torch.distributions.Categorical(probs=probs)\n        if deterministic:\n            probs = torch.softmax(logits, dim=-1)\n            action = torch.argmax(probs, dim=-1)\n        if action is None:\n            action = dist.sample()\n        log_prob = dist.log_prob(action)\n        entropy = dist.entropy()\n        return action, log_prob, entropy\n    \n    def evaluate_get_action(self, x, action):\n        features = self.get_features(x)\n        logits = self.actor(features)\n        dist = torch.distributions.Categorical(logits=logits)\n        log_prob = dist.log_prob(action)\n        entropy = dist.entropy()\n        return log_prob, entropy\n\n# --- Environment Creation ---\ndef make_env(env_id, seed, idx, run_name, eval_mode=False):\n    def thunk():\n        render_mode = \"rgb_array\" if eval_mode else None\n        # Force RGB24 format for ViZDoom to avoid CRCGCB warning\n        env = gym.make(env_id, render_mode=render_mode)\n        env = gym.wrappers.RecordEpisodeStatistics(env)\n        # env = gym.wrappers.AtariPreprocessing(env,\n        #     frame_skip=4,  # Standard frame skip for Atari\n        #     grayscale_obs=True,  # Add channel dimension for grayscale\n        #     scale_obs=True,  # Scale observations to [0, 1]\n        #     screen_size=(TARGET_HEIGHT, TARGET_WIDTH),  # Resize to target dimensions\n        # )\n        # # Use our custom wrapper for all preprocessing\n        # # env = PreprocessAndFrameStack(env, height=TARGET_HEIGHT, width=TARGET_WIDTH, num_stack=4)\n        # env = gym.wrappers.FrameStackObservation(env, 4)\n        \n        # env = gym.wrappers.RecordEpisodeStatistics(env)\n        # env = NoopResetEnv(env, noop_max=30)\n        # env = MaxAndSkipEnv(env, skip=4)\n        # env = EpisodicLifeEnv(env)  \n        # # print(env.unwrapped.get_action_meanings())\n        # if \"FIRE\" in env.unwrapped.get_action_meanings():\n        #     env = FireResetEnv(env)\n        # env = ClipRewardEnv(env)\n        # env = gym.wrappers.ResizeObservation(env, (84, 84))\n        # env = gym.wrappers.GrayscaleObservation(env)\n        # env = gym.wrappers.FrameStackObservation(env, 4)\n         # Use the all-in-one, official Atari wrapper\n        env = gym.wrappers.AtariPreprocessing(\n            env,\n            noop_max=30,\n            frame_skip=4,\n            screen_size=84, # It assumes square images\n            terminal_on_life_loss=True, # Standard for training\n            grayscale_obs=True,\n            scale_obs=True # We want uint8 [0, 255] for storage\n        )\n        \n        # Now, stack the preprocessed frames\n        # env = ClipRewardEnv(env)  # Clip rewards to [-1, 1]\n        env = gym.wrappers.FrameStackObservation(env, 4)\n        env.action_space.seed(seed + idx)\n        env.observation_space.seed(seed + idx)\n        return env\n    return thunk\n\n# --- Evaluation ---\ndef evaluate(agent_model, device, run_name, num_eval_eps=10, record=False):\n    eval_env = make_env(env_id=Config.env_id, seed=Config.seed, idx=0, run_name=run_name, eval_mode=True)()\n    \n    agent_model.to(device)\n    agent_model.eval()\n    returns = []\n    frames = []\n\n    for eps in tqdm(range(num_eval_eps), desc=\"Evaluating\"):\n        obs, _ = eval_env.reset()\n        done = False\n        episode_reward = 0.0\n\n        while not done:\n            if record:\n                # Get the raw frame from the original env for nice videos\n                frame = eval_env.unwrapped.render()\n                frames.append(frame)\n\n            with torch.no_grad():\n                # Add batch dimension and convert to tensor\n                obs_tensor = torch.tensor(obs, device=device, dtype=torch.float32).unsqueeze(0)\n                action, _, _ = agent_model.get_action(obs_tensor, deterministic=True)\n                # Convert action to scalar integer for ViZDoom\n                action_scalar = action.cpu().numpy().item()\n                obs, reward, terminated, truncated, info = eval_env.step(action_scalar)\n                done = terminated or truncated\n                episode_reward += float(reward)\n                # Use raw reward from info if available\n                # if \"episode\" in info:\n                #     episode_reward = info[\"episode\"][\"r\"]\n          \n        returns.append(episode_reward)\n      \n    eval_env.close()\n    agent_model.train()\n    return returns, frames\n\n\n\n# --- Main Execution ---\nif __name__ == \"__main__\":\n    args = Config()\n    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n\n    if args.use_wandb:\n        wandb.init(\n            project=args.wandb_project,\n            sync_tensorboard=True,\n            config=vars(args),\n            name=run_name,\n            monitor_gym=True,\n            save_code=True,\n        )\n    # writer = SummaryWriter(f\"runs/{run_name}\")\n    \n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    device = \"cuda\"\n\n    envs = gym.vector.SyncVectorEnv(\n        [make_env(args.env_id, args.seed, i, run_name) for i in range(args.num_envs)]\n    )\n    # assert isinstance(envs.single_action_space, gym.spaces.Box), \"only continuous action space is supported\"\n\n    actor_network = Agent(envs.single_action_space.n).to(device)\n    optimizer = optim.Adam(actor_network.parameters(), lr=args.lr, eps=1e-5)\n    # critic_optim = optim.Adam(critic_network.parameters(), lr=args.lr, eps=1e-5)\n\n    obs_storage = torch.zeros((args.max_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n    actions_storage = torch.zeros((args.max_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n    logprobs_storage = torch.zeros((args.max_steps, args.num_envs)).to(device)\n    rewards_storage = torch.zeros((args.max_steps, args.num_envs)).to(device)\n    dones_storage = torch.zeros((args.max_steps, args.num_envs)).to(device)\n    values_storage = torch.zeros((args.max_steps, args.num_envs)).to(device)\n    \n    global_step = 0\n    start_time = time.time()\n    num_updates = args.total_timesteps // args.batch_size\n    \n    next_obs, _ = envs.reset(seed=args.seed)\n    next_obs = torch.Tensor(next_obs).to(device)\n    next_done = torch.zeros(args.num_envs).to(device)\n\n    for update in tqdm(range(1, num_updates + 1), desc=\"Training Updates\"):\n        \n        frac = 1.0 - (update / num_updates)\n        lr = args.lr * frac\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = lr\n        \n      \n        \n        for step in range(0, args.max_steps):\n            global_step += args.num_envs\n            obs_storage[step] = next_obs\n            dones_storage[step] = next_done\n\n            with torch.no_grad():\n                action, logprob, _ = actor_network.get_action(next_obs)\n                value = actor_network.get_value(next_obs)\n            \n            values_storage[step] = value.flatten()\n            actions_storage[step] = action\n            logprobs_storage[step] = logprob\n\n            next_obs, reward, terminated, truncated, info = envs.step(action.cpu().numpy())\n            done = np.logical_or(terminated, truncated)\n            \n            rewards_storage[step] = torch.tensor(reward).to(device).view(-1)\n            next_obs = torch.Tensor(next_obs).to(device)\n            next_done = torch.Tensor(done).to(device)\n\n            if \"final_info\" in info:\n                for item in info[\"final_info\"]:\n                    if item and \"episode\" in item:\n                        wandb.log({\"charts/episodic_return\": item['episode']['r'], \"global_step\": global_step})\n                        wandb.log({\"charts/episodic_length\": item['episode']['l'], \"global_step\": global_step})\n\n        # === Advantage Calculation & Returns (YOUR ORIGINAL LOGIC) ===\n        with torch.no_grad():\n            advantages = torch.zeros_like(rewards_storage).to(device)\n            \n            # 1. Bootstrap value: Get value of the state *after*\n            bootstrap_value = actor_network.get_value(next_obs).squeeze()\n            lastgae = 0.0\n\n            for t in reversed(range(args.max_steps)):\n                \n                if t == args.max_steps - 1:\n                    nextnonterminal = (1.0 - next_done)\n                    gt_next_state = bootstrap_value * nextnonterminal\n                else:\n                    nextnonterminal = (1.0 - dones_storage[t + 1])\n                    gt_next_state = values_storage[t + 1] * nextnonterminal # If done at t, the next gt is 0\n                \n                delta = (rewards_storage[t] +  args.gamma *  gt_next_state ) - values_storage[t]\n\n                advantages[t] = lastgae = delta + args.GAE * lastgae * nextnonterminal * args.gamma\n\n        \n        # Calculate advantages using the computed returns and stored values\n        returns = advantages + values_storage\n        # advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n\n        # === PPO Update Phase ===\n        b_obs = obs_storage.reshape((-1,) +  envs.single_observation_space.shape)\n        b_logprobs = logprobs_storage.reshape(-1)\n        b_actions = actions_storage.reshape((-1,) + envs.single_action_space.shape)\n        b_advantages = advantages.reshape(-1)\n        b_returns = returns.reshape(-1)\n        b_values = values_storage.reshape(-1)\n\n        b_inds = np.arange(args.batch_size)\n        for epoch in range(args.PPO_EPOCHS):\n            np.random.shuffle(b_inds)\n            \n        \n            \n            for start in range(0, args.batch_size, args.minibatch_size):\n                end = start + args.minibatch_size\n                mb_inds = b_inds[start:end]\n\n                new_log_probs, entropy = actor_network.evaluate_get_action(b_obs[mb_inds], b_actions[mb_inds])\n                ratio = torch.exp(new_log_probs - b_logprobs[mb_inds])\n                logratio = new_log_probs - b_logprobs[mb_inds]\n                with torch.no_grad():\n                    approx_kl = ((ratio - 1) - logratio).mean()\n                    wandb.log({\"charts/approx_kl\": approx_kl.item()})\n\n                b_advantages[mb_inds] = (b_advantages[mb_inds] - b_advantages[mb_inds].mean()) / (b_advantages[mb_inds].std() + 1e-8)\n                \n                pg_loss1 = b_advantages[mb_inds] * ratio\n                pg_loss2 = b_advantages[mb_inds] * torch.clamp(ratio, 1 - args.clip_value, 1 + args.clip_value)\n                policy_loss = -torch.min(pg_loss1, pg_loss2).mean()\n\n                current_values = actor_network.get_value(b_obs[mb_inds]).squeeze()\n                \n                # Value clipping\n                v_loss_unclipped = (current_values - b_returns[mb_inds]) ** 2\n                v_clipped = b_values[mb_inds] + torch.clamp(\n                    current_values - b_values[mb_inds], -args.clip_coeff, args.clip_coeff\n                )\n                v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n                critic_loss = args.VALUE_COEFF * 0.5 * v_loss_max.mean()\n                \n                entropy_loss = entropy.mean()\n                loss = policy_loss - args.ENTROPY_COEFF * entropy_loss + critic_loss\n\n                # actor_optim.zero_grad()\n                optimizer.zero_grad()\n                loss.backward()\n                \n                grad_norm_dict = {}\n                total_norm = 0\n                for name, param in actor_network.named_parameters():\n                    if param.grad is not None:\n                        param_norm = param.grad.data.norm(2)\n                        if 'actor' in name or 'critic' in name:\n                            grad_norm_dict[f\"gradients/norm_{name}\"] = param_norm.item()\n                        else:\n                            grad_norm_dict[f\"gradients/shared_norm_{name}\"] = param_norm.item()\n                        total_norm += param_norm.item() ** 2\n                grad_norm_dict[\"gradients/total_norm\"] = total_norm ** 0.5\n                wandb.log(grad_norm_dict)\n                \n                nn.utils.clip_grad_norm_(actor_network.parameters(), 0.5)\n                # nn.utils.clip_grad_norm_(actor_network.parameters(), 0.5)\n                # actor_optim.step()\n                optimizer.step()\n        \n        if args.use_wandb:\n            wandb.log({ \n                \"losses/total_loss\": loss.item(),\n                \"losses/policy_loss\": policy_loss.item(),\n                \"losses/value_loss\": critic_loss.item(),\n                \"losses/entropy\": entropy_loss.item(),\n                \"charts/learning_rate\": optimizer.param_groups[0]['lr'],\n                \"charts/episodic_return\": np.mean(rewards_storage.cpu().numpy()),\n                \"charts/advantages_mean\": b_advantages.mean().item(),\n                \"charts/advantages_std\": b_advantages.std().item(),\n                \"charts/returns_mean\": b_returns.mean().item(),\n                \"global_step\": global_step,\n            })\n            print(f\"Update {update}, Global Step: {global_step}, Policy Loss: {policy_loss.item():.4f}, Value Loss: {critic_loss.item():.4f}\")\n    \n        if update % 200 == 0:\n            episodic_returns, _ = evaluate(actor_network, device, run_name, num_eval_eps=5, record=args.capture_video)\n            # Log the average return from the evaluation\n            avg_return = np.mean(episodic_returns)\n            \n            if args.use_wandb:\n                wandb.log({\n                    \"eval/avg_return\": avg_return,\n                    \"global_step\": global_step,\n                })\n            print(f\"Evaluation at step {global_step}: Average raw return = {avg_return:.2f}\")\n\n    if args.capture_video:\n        print(\"Capturing final evaluation video...\")\n        episodic_returns, eval_frames = evaluate(actor_network, device, run_name, num_eval_eps=10, record=True)\n\n        if len(eval_frames) > 0:\n            video_path = f\"videos/final_eval_{run_name}.mp4\"\n            os.makedirs(os.path.dirname(video_path), exist_ok=True)\n            imageio.mimsave(video_path, eval_frames, fps=30, codec='libx264')\n            if args.use_wandb:\n                wandb.log({\"eval/final_video\": wandb.Video(video_path, fps=30, format=\"mp4\")})\n                print(f\"Final evaluation video saved and uploaded to WandB.\")\n\n    envs.close()\n    if args.use_wandb:\n        wandb.finish()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb\nwandb.login(key=secret_value_0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install  SuperSuit","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pettingzoo[atari]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall stable_baselines3 -y","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install stable_baselines3==2.6.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport time\nimport gymnasium as gym\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport wandb\nimport imageio\nimport ale_py\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom stable_baselines3.common.env_util import make_vec_env\nfrom stable_baselines3.common.callbacks import EvalCallback\nfrom stable_baselines3.common.policies import ActorCriticPolicy\nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n\nfrom wandb.integration.sb3 import WandbCallback\n\ngym.register_envs(ale_py)\n\n# ===== CONFIGURATION (Mirrors the custom script) =====\nclass Config:\n    # Experiment settings\n    exp_name = \"PPO-SB3-Atari-Benchmark\"\n    seed = 42\n    env_id = \"BoxingNoFrameskip-v4\"\n    total_timesteps = 10_000_000\n\n    # PPO & Agent settings\n    lr = 2.5e-4\n    gamma = 0.99\n    num_envs = 8  # Number of parallel environments\n    n_steps = 128  # Steps per rollout per environment (max_steps in custom)\n    num_minibatches = 4\n    n_epochs = 4   # PPO_EPOCHS in custom\n    clip_range = 0.1 # clip_value in custom\n    ent_coef = 0.01  # ENTROPY_COEFF\n    vf_coef = 0.5    # VALUE_COEFF\n    gae_lambda = 0.95 # GAE\n    max_grad_norm = 0.5\n    \n    # Logging & Saving\n    capture_video = True\n    use_wandb = True\n    wandb_project = \"cleanRL\"\n    \n    # Evaluation\n    eval_freq_updates = 200 # Evaluate every 200 updates\n    num_eval_episodes = 10\n\n    # Derived values\n    @property\n    def batch_size(self):\n        # In SB3, this is the minibatch size.\n        return (self.num_envs * self.n_steps) // self.num_minibatches\n    \n    @property\n    def eval_freq_steps(self):\n        # Convert update frequency to step frequency\n        return self.eval_freq_updates * self.n_steps\n\n# --- Environment Creation (Mirrors the custom script) ---\ndef make_env(env_id, seed, idx):\n    # def thunk():\n    env = gym.make(env_id)\n    # Use the all-in-one, official Atari wrapper from Gymnasium\n    # This handles: No-op resets, frame skipping, resizing, grayscaling, life-based terminals, and reward clipping.\n    env = gym.wrappers.AtariPreprocessing(\n        env,\n        noop_max=30,\n        frame_skip=4,\n        screen_size=84,\n        terminal_on_life_loss=True,\n        grayscale_obs=True,\n        scale_obs=True # Keep as False to match custom script's uint8 storage, SB3 handles scaling\n    )\n    # Stack the preprocessed frames\n    env = gym.wrappers.FrameStackObservation(env, 4)\n    env.action_space.seed(seed + idx)\n    env.observation_space.seed(seed + idx)\n    return env\n    # return thunk\n\n# --- Custom Network Architecture (Mirrors the custom script) ---\ndef layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n    torch.nn.init.orthogonal_(layer.weight, std)\n    torch.nn.init.constant_(layer.bias, bias_const)\n    return layer\n\nclass CustomCNN(BaseFeaturesExtractor):\n    \"\"\"\n    Custom CNN feature extractor to match the architecture of the custom PyTorch script.\n    \"\"\"\n    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 512):\n        super(CustomCNN, self).__init__(observation_space, features_dim)\n        # The observation space from FrameStack is (4, 84, 84)\n        n_input_channels = observation_space.shape[0]\n        self.cnn = nn.Sequential(\n            layer_init(nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4)),\n            nn.ReLU(),\n            layer_init(nn.Conv2d(32, 64, kernel_size=4, stride=2)),\n            nn.ReLU(),\n            layer_init(nn.Conv2d(64, 64, kernel_size=3, stride=1)),\n            nn.ReLU(),\n            nn.Flatten(),\n        )\n\n        # Compute shape by doing one forward pass\n        with torch.no_grad():\n            n_flatten = self.cnn(\n                torch.as_tensor(observation_space.sample()[None]).float()\n            ).shape[1]\n\n        self.linear = nn.Sequential(\n            layer_init(nn.Linear(n_flatten, features_dim)),\n            nn.ReLU()\n        )\n\n    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n        # SB3 automatically handles the normalization of images (dividing by 255)\n        return self.linear(self.cnn(observations))\n\n# ===== SCRIPT START =====\nif __name__ == '__main__':\n    # --- Setup ---\n    args = Config()\n    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n    \n    if args.use_wandb:\n        run = wandb.init(\n            project=args.wandb_project,\n            sync_tensorboard=True,\n            config=vars(args),\n            name=run_name,\n            monitor_gym=True,\n            save_code=True,\n        )\n\n    # --- Set seeds for reproducibility ---\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    \n    # --- Create Vectorized Environment ---\n    env = DummyVecEnv([make_env(args.env_id, args.seed, i) for i in range(args.num_envs)])\n\n    # --- Define Hyperparameters and Policy Architecture ---\n    policy_kwargs = {\n        \"features_extractor_class\": CustomCNN,\n        \"features_extractor_kwargs\": {\"features_dim\": 512},\n        \"net_arch\": [],  # No extra hidden layers between extractor and heads\n        \"activation_fn\": nn.ReLU,\n    }\n\n    # --- Create PPO Model ---\n    model = PPO(\n        \"CnnPolicy\",\n        env,\n        policy_kwargs=policy_kwargs,\n        learning_rate=args.lr,\n        n_steps=args.n_steps,\n        batch_size=args.batch_size,\n        n_epochs=args.n_epochs,\n        gamma=args.gamma,\n        gae_lambda=args.gae_lambda,\n        clip_range=args.clip_range,\n        ent_coef=args.ent_coef,\n        vf_coef=args.vf_coef,\n        max_grad_norm=args.max_grad_norm,\n        seed=args.seed,\n        tensorboard_log=f\"runs/{run.id}\" if args.use_wandb else None,\n        verbose=1,\n    )\n\n    # --- Setup Callbacks ---\n    callbacks = []\n    # 1. Evaluation Callback\n    # Create a separate, non-vectorized env for evaluation\n    eval_env = make_env(args.env_id, args.seed, 0)()\n    eval_callback = EvalCallback(\n        eval_env,\n        best_model_save_path=f'models/{run.id}/' if args.use_wandb else None,\n        log_path=f'models/{run.id}/' if args.use_wandb else None,\n        eval_freq=max(args.eval_freq_steps // args.num_envs, 1),\n        n_eval_episodes=args.num_eval_episodes,\n        deterministic=True,\n        render=False,\n    )\n    callbacks.append(eval_callback)\n\n    # 2. W&B Callback\n    if args.use_wandb:\n        wandb_callback = WandbCallback(\n            gradient_save_freq=100_000, # Log gradients periodically\n            model_save_path=f\"models/{run.id}\",\n            verbose=2,\n        )\n        callbacks.append(wandb_callback)\n\n    # --- Train ---\n    print(\"Policy Architecture:\")\n    print(model.policy)\n    print(\"\\nStarting training...\")\n    model.learn(\n        total_timesteps=args.total_timesteps,\n        callback=callbacks,\n        progress_bar=True,\n    )\n\n\n    # --- Cleanup ---\n    env.close()\n    eval_env.close()\n    if 'video_env' in locals():\n        video_env.close()\n    if args.use_wandb:\n        run.finish()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nimport os\nimport random\nimport time\nimport gymnasium as gym\nfrom tqdm import tqdm\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nimport wandb\nimport cv2\nimport imageio\n# import ale_py\nfrom pettingzoo.atari import pong_v3\nimport importlib\nimport supersuit as ss\nfrom pettingzoo.atari import pong_v3\n\n# ===== CONFIGURATION =====\nclass Config:\n    # Experiment settings\n    exp_name = \"PPO-PettingZoo-Pong\"\n    seed = 42\n    env_id = \"pong_v3\"\n    total_timesteps = 5_000_000  # Standard metric for vectorized training\n\n    # PPO & Agent settings\n    lr = 2.5e-4\n    gamma = 0.99\n    num_envs = 8  # Number of parallel environments\n    max_steps = 128  # Steps per rollout per environment (aka num_steps)\n    num_minibatches = 4\n    PPO_EPOCHS = 4\n    clip_value = 0.1 \n    clip_coeff = 0.1  # Value clipping coefficient\n    ENTROPY_COEFF = 0.01\n    \n    VALUE_COEFF = 0.5\n    \n    # Logging & Saving\n    capture_video = False\n    use_wandb = True\n    wandb_project = \"cleanRL\"\n    \n    GAE = 0.95  # Generalized Advantage Estimation\n    anneal_lr = True  # Whether to linearly decay the learning rate\n    max_grad_norm = 0.5  # Gradient clipping value\n    \n    \n    # Derived values\n    @property\n    def batch_size(self):\n        return self.num_envs * self.max_steps\n\n    @property\n    def minibatch_size(self):\n        return self.batch_size // self.num_minibatches\n\n# --- Preprocessing ---\nTARGET_HEIGHT = 64\nTARGET_WIDTH = 64\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# --- Networks ---\ndef layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n    torch.nn.init.orthogonal_(layer.weight, std)\n    torch.nn.init.constant_(layer.bias, bias_const)\n    return layer\n\nclass Agent(nn.Module):\n    def __init__(self, action_space):\n        super(Agent, self).__init__()\n        # Shared CNN feature extractor\n        self.network = nn.Sequential(\n            layer_init(nn.Conv2d(6, 32, kernel_size=8, stride=4)),\n            nn.ReLU(),\n            layer_init(nn.Conv2d(32, 64, kernel_size=4, stride=2)),\n            nn.ReLU(),\n            layer_init(nn.Conv2d(64, 64, kernel_size=3, stride=1)),\n            nn.ReLU(),\n            nn.Flatten(),\n            layer_init(nn.Linear(64 * 7 * 7, 512)), # Adjusted for 64x64 input\n            nn.ReLU(),\n        )\n        # Actor head\n        self.actor = layer_init(nn.Linear(512, action_space), std=0.01)\n        # Critic head\n        self.critic = layer_init(nn.Linear(512, 1), std=1.0)\n\n    def get_features(self, x):\n        return self.network(x)\n\n    def get_value(self, x):  \n        x = x.clone()\n        x = x.permute(0, 3, 1, 2)\n        x[:, :4, :, :] /= 255.0\n        return self.critic(self.get_features(x))\n\n    def get_action(self, x, action=None, deterministic=False):\n        # print(\"No eval: \", x.shape)\n        x = x.clone()\n        x = x.permute(0, 3, 1, 2)\n        x[:, :4, :, :] /= 255.0\n        \n        features = self.get_features(x)\n        logits = self.actor(features)\n        probs = torch.softmax(logits, dim=-1)\n        dist = torch.distributions.Categorical(probs=probs)\n        if deterministic:\n            probs = torch.softmax(logits, dim=-1)\n            action = torch.argmax(probs, dim=-1)\n        if action is None:\n            action = dist.sample()\n        log_prob = dist.log_prob(action)\n        entropy = dist.entropy()\n        return action, log_prob, entropy\n    \n    def evaluate_get_action(self, x, action):\n        # print(\"Eval: \", x.shape)\n        x = x.clone()\n        x = x.permute(0, 3, 1, 2)\n        x[:, :4, :, :] /= 255.0\n        features = self.get_features(x)\n        logits = self.actor(features)\n        dist = torch.distributions.Categorical(logits=logits)\n        log_prob = dist.log_prob(action)\n        entropy = dist.entropy()\n        return log_prob, entropy\n\n# --- Environment Creation ---\ndef make_env(env_id, seed, idx, run_name, eval_mode=False):\n    \n    env = importlib.import_module(f\"pettingzoo.atari.{args.env_id}\").parallel_env()\n    # env = gym.wrappers.RecordEpisodeStatistics(env)\n    # env.reset(seed=seed)  # <--- Required to initialize np_random\n    env = ss.max_observation_v0(env, 2)\n    # env = ss.frame_skip_v0(env, 4)\n    env = ss.clip_reward_v0(env, lower_bound=-1, upper_bound=1)\n    env = ss.color_reduction_v0(env, mode=\"B\")\n    env = ss.resize_v1(env, x_size=84, y_size=84)\n    env = ss.frame_stack_v1(env, 4)\n    env = ss.agent_indicator_v0(env, type_only=False)\n    env = ss.pettingzoo_env_to_vec_env_v1(env)\n    envs = ss.concat_vec_envs_v1(env, args.num_envs // 2, num_cpus=0, base_class=\"gymnasium\")\n    envs.single_observation_space = envs.observation_space\n    envs.single_action_space = envs.action_space\n    envs.is_vector_env = True\n    # envs = gym.wrappers.RecordEpisodeStatistics(envs)\n  \n    return envs\n\n\n# --- NEW AND SIMPLE evaluate function ---\ndef evaluate(model, device, run_name, num_eval_eps=10, record=False):\n    # For evaluation, we create a single, NON-vectorized game.\n    # We use the .aec_env() which is designed for turn-by-turn interaction.\n    eval_env = pong_v3.env(render_mode=\"rgb_array\" if record else None)\n    \n    # Apply the same wrappers as the training env, but without the vectorization ones.\n    eval_env = ss.max_observation_v0(eval_env, 2)\n    # No frame_skip for evaluation, we want to see all frames.\n    eval_env = ss.clip_reward_v0(eval_env, lower_bound=-1, upper_bound=1)\n    eval_env = ss.color_reduction_v0(eval_env, mode=\"B\")\n    eval_env = ss.resize_v1(eval_env, x_size=84, y_size=84)\n    eval_env = ss.frame_stack_v1(eval_env, 4)\n    eval_env = ss.agent_indicator_v0(eval_env, type_only=False)\n\n    model.to(device)\n    model.eval()\n    \n    all_returns = []\n    frames = []\n\n    for _ in tqdm(range(num_eval_eps), desc=\"Evaluating\"):\n        eval_env.reset(seed=Config.seed + len(all_returns))\n        episode_return = 0.0\n        \n        # The main AEC loop\n        for agent in eval_env.agent_iter():\n            obs, reward, terminated, truncated, info = eval_env.last()\n            done = terminated or truncated\n\n            if done:\n                # When an agent is done, we need to step one last time with a None action\n                eval_env.step(None)\n                continue\n\n            # Accumulate reward for the current agent\n            episode_return += reward\n\n            # If it's our trained agent's turn\n            if agent == 'first_0':\n                with torch.no_grad():\n                    # The observation needs a batch dimension for the network\n                    obs_tensor = torch.Tensor(obs).to(device).unsqueeze(0)\n                    action, _, _ = model.get_action(obs_tensor, deterministic=True)\n                eval_env.step(action.cpu().item())\n            else:\n                # The opponent just takes a random action\n                action = eval_env.action_space(agent).sample()\n                eval_env.step(action)\n            \n            if record:\n                frames.append(eval_env.render())\n\n        all_returns.append(episode_return)\n\n    eval_env.close()\n    model.train()\n    \n    return all_returns, frames\n# --- Main Execution ---\nif __name__ == \"__main__\":\n    args = Config()\n    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n\n    if args.use_wandb:\n        wandb.init(\n            project=args.wandb_project,\n            sync_tensorboard=True,\n            config=vars(args),\n            name=run_name,\n            monitor_gym=True,\n            save_code=True,\n        )\n    # writer = SummaryWriter(f\"runs/{run_name}\")\n    \n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    device = \"cuda\"\n\n    # 3. Create multiple parallel games\n    envs = make_env(env_id=args.env_id, seed=args.seed, idx=0, run_name=run_name)\n    # print(env.single_action_space)\n    # envs = ss.concat_vec_envs_v1(env, args.num_envs // 2, num_cpus=0, base_class=\"gymnasium\")\n    \n    actor_network = Agent(envs.action_space.n).to(device)\n    \n    optimizer = optim.Adam(actor_network.parameters(), lr=args.lr, eps=1e-5)\n    # critic_optim = optim.Adam(critic_network.parameters(), lr=args.lr, eps=1e-5)\n\n    obs_storage = torch.zeros((args.max_steps, args.num_envs) + envs.observation_space.shape).to(device)\n    actions_storage = torch.zeros((args.max_steps, args.num_envs) + envs.action_space.shape).to(device)\n    logprobs_storage = torch.zeros((args.max_steps, args.num_envs)).to(device)\n    rewards_storage = torch.zeros((args.max_steps, args.num_envs)).to(device)\n    dones_storage = torch.zeros((args.max_steps, args.num_envs)).to(device)\n    values_storage = torch.zeros((args.max_steps, args.num_envs)).to(device)\n    \n    # Episode tracking variables\n    episodic_return = np.zeros(args.num_envs)\n    episode_step_count = np.zeros(args.num_envs)\n    \n    global_step = 0\n    start_time = time.time()\n    num_updates = args.total_timesteps // args.batch_size\n    \n    next_obs, _ = envs.reset(seed=args.seed)\n    next_obs = torch.Tensor(next_obs).to(device)\n    next_done = torch.zeros(args.num_envs).to(device)\n\n    for update in tqdm(range(1, num_updates + 1), desc=\"Training Updates\"):\n        \n        frac = 1.0 - (update / num_updates)\n        lr = args.lr * frac\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = lr\n        \n      \n        \n        for step in range(0, args.max_steps):\n            global_step += args.num_envs\n            obs_storage[step] = next_obs\n            dones_storage[step] = next_done\n\n            # for agent in envs.agent_iter():\n                \n            \n            # if done:\n            #     envs.step(None)\n            #     continue\n            \n            # elif agent == 'first_0':\n            \n            with torch.no_grad():\n                action, logprob, _ = actor_network.get_action(next_obs)\n                value = actor_network.get_value(next_obs)\n            \n            values_storage[step] = value.flatten()\n            actions_storage[step] = action\n            logprobs_storage[step] = logprob\n\n            next_obs, reward, terminated, truncated, info = envs.step(action.cpu().numpy())\n            done = np.logical_or(terminated, truncated)\n            \n            # Update episode tracking\n            episodic_return += reward\n            episode_step_count += 1\n            \n            rewards_storage[step] = torch.tensor(reward).to(device).view(-1)\n            next_obs = torch.Tensor(next_obs).to(device)\n            next_done = torch.Tensor(done).to(device)\n            # else:\n            #     # For other agents, just sample random actions\n            #     action = envs.single_action_space(agent).sample()\n            #     envs.step(action)\n            #     rewards_storage[step] = torch.tensor(reward).to(device).view(-1)\n                # next_obs = torch.Tensor(obs).to(device)\n                # next_done = torch.Tensor(done).to(device)\n                \n                \n            if \"final_info\" in info:\n                for item in info[\"final_info\"]:\n                    # The item can be None if the env at that index is not done\n                    if item and \"episode\" in item:\n                        print(f\"global_step={global_step}, episodic_return={item['episode']['r']}\")\n                        wandb.log({\n                            \"charts/episodic_return\": item['episode']['r'],\n                            \"charts/episodic_length\": item['episode']['l'],\n                            \"global_step\": global_step\n                        })\n        # === Advantage Calculation & Returns (YOUR ORIGINAL LOGIC) ===\n        with torch.no_grad():\n            advantages = torch.zeros_like(rewards_storage).to(device)\n            \n            # 1. Bootstrap value: Get value of the state *after*\n            bootstrap_value = actor_network.get_value(next_obs).squeeze()\n            lastgae = 0.0\n\n            for t in reversed(range(args.max_steps)):\n                \n                if t == args.max_steps - 1:\n                    nextnonterminal = (1.0 - next_done)\n                    gt_next_state = bootstrap_value * nextnonterminal\n                else:\n                    nextnonterminal = (1.0 - dones_storage[t + 1])\n                    gt_next_state = values_storage[t + 1] * nextnonterminal # If done at t, the next gt is 0\n                \n                delta = (rewards_storage[t] +  args.gamma *  gt_next_state ) - values_storage[t]\n\n                advantages[t] = lastgae = delta + args.GAE * lastgae * nextnonterminal * args.gamma\n\n        \n        # Calculate advantages using the computed returns and stored values\n        returns = advantages + values_storage\n        # advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n\n        # === PPO Update Phase ===\n        b_obs = obs_storage.reshape((-1,) +  envs.observation_space.shape)\n        b_logprobs = logprobs_storage.reshape(-1)\n        b_actions = actions_storage.reshape((-1,) + envs.action_space.shape)\n        b_advantages = advantages.reshape(-1)\n        b_returns = returns.reshape(-1)\n        b_values = values_storage.reshape(-1)\n\n        b_inds = np.arange(args.batch_size)\n        for epoch in range(args.PPO_EPOCHS):\n            np.random.shuffle(b_inds)\n            \n        \n            \n            for start in range(0, args.batch_size, args.minibatch_size):\n                end = start + args.minibatch_size\n                mb_inds = b_inds[start:end]\n\n                new_log_probs, entropy = actor_network.evaluate_get_action(b_obs[mb_inds], b_actions[mb_inds])\n                ratio = torch.exp(new_log_probs - b_logprobs[mb_inds])\n                logratio = new_log_probs - b_logprobs[mb_inds]\n                with torch.no_grad():\n                    approx_kl = ((ratio - 1) - logratio).mean()\n                    wandb.log({\"charts/approx_kl\": approx_kl.item()})\n\n                b_advantages[mb_inds] = (b_advantages[mb_inds] - b_advantages[mb_inds].mean()) / (b_advantages[mb_inds].std() + 1e-8)\n                \n                pg_loss1 = b_advantages[mb_inds] * ratio\n                pg_loss2 = b_advantages[mb_inds] * torch.clamp(ratio, 1 - args.clip_value, 1 + args.clip_value)\n                policy_loss = -torch.min(pg_loss1, pg_loss2).mean()\n\n                current_values = actor_network.get_value(b_obs[mb_inds]).squeeze()\n                \n                # Value clipping\n                v_loss_unclipped = (current_values - b_returns[mb_inds]) ** 2\n                v_clipped = b_values[mb_inds] + torch.clamp(\n                    current_values - b_values[mb_inds], -args.clip_coeff, args.clip_coeff\n                )\n                v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n                critic_loss = args.VALUE_COEFF * 0.5 * v_loss_max.mean()\n                \n                entropy_loss = entropy.mean()\n                loss = policy_loss - args.ENTROPY_COEFF * entropy_loss + critic_loss\n\n                # actor_optim.zero_grad()\n                optimizer.zero_grad()\n                loss.backward()\n                \n                grad_norm_dict = {}\n                total_norm = 0\n                for name, param in actor_network.named_parameters():\n                    if param.grad is not None:\n                        param_norm = param.grad.data.norm(2)\n                        if 'actor' in name or 'critic' in name:\n                            grad_norm_dict[f\"gradients/norm_{name}\"] = param_norm.item()\n                        else:\n                            grad_norm_dict[f\"gradients/shared_norm_{name}\"] = param_norm.item()\n                        total_norm += param_norm.item() ** 2\n                grad_norm_dict[\"gradients/total_norm\"] = total_norm ** 0.5\n                wandb.log(grad_norm_dict)\n                \n                nn.utils.clip_grad_norm_(actor_network.parameters(), 0.5)\n                # nn.utils.clip_grad_norm_(actor_network.parameters(), 0.5)\n                # actor_optim.step()\n                optimizer.step()\n        \n        if args.use_wandb:\n            wandb.log({ \n                \"losses/total_loss\": loss.item(),\n                \"losses/policy_loss\": policy_loss.item(),\n                \"losses/value_loss\": critic_loss.item(),\n                \"losses/entropy\": entropy_loss.item(),\n                \"charts/learning_rate\": optimizer.param_groups[0]['lr'],\n                \"charts/episodic_return\": np.mean(rewards_storage.cpu().numpy()),\n                \"charts/advantages_mean\": b_advantages.mean().item(),\n                \"charts/advantages_std\": b_advantages.std().item(),\n                \"charts/returns_mean\": b_returns.mean().item(),\n                \"global_step\": global_step,\n            })\n            print(f\"Update {update}, Global Step: {global_step}, Policy Loss: {policy_loss.item():.4f}, Value Loss: {critic_loss.item():.4f}\")\n    \n        if update % 200 == 0:\n            episodic_returns, _ = evaluate(actor_network, device, run_name, num_eval_eps=5, record=args.capture_video)\n            # Log the average return from the evaluation\n            avg_return = np.mean(episodic_returns)\n            \n            if args.use_wandb:\n                wandb.log({\n                    \"eval/avg_return\": avg_return,\n                    \"global_step\": global_step,\n                })\n            print(f\"Evaluation at step {global_step}: Average raw return = {avg_return:.2f}\")\n\n    if args.capture_video:\n        print(\"Capturing final evaluation video...\")\n        episodic_returns, eval_frames = evaluate(actor_network, device, run_name, num_eval_eps=10, record=True)\n\n        if len(eval_frames) > 0:\n            video_path = f\"videos/final_eval_{run_name}.mp4\"\n            os.makedirs(os.path.dirname(video_path), exist_ok=True)\n            imageio.mimsave(video_path, eval_frames, fps=30, codec='libx264')\n            if args.use_wandb:\n                wandb.log({\"eval/final_video\": wandb.Video(video_path, fps=30, format=\"mp4\")})\n                print(f\"Final evaluation video saved and uploaded to WandB.\")\n\n    envs.close()\n    if args.use_wandb:\n        wandb.finish()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install autorom","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install supersuit","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!AutoROM --accept-license","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}