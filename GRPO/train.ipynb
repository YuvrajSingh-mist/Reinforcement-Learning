{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3207f0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd4a7f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import reasoning_gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78474ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class GRPOConfig:\n",
    "    \"\"\"Configuration class for GRPO (Generalized Reward-based Policy Optimization) hyperparameters\"\"\"\n",
    "    \n",
    "    # Model and training hyperparameters\n",
    "    model_name: str = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "    learning_rate: float = 3e-4\n",
    "    batch_size: int = 32\n",
    "    num_updates: int = 1000\n",
    "    max_steps: int = 20\n",
    "    n_outputs: int = 4\n",
    "    max_length: int = 256\n",
    "    grpo_iterations: int = 4  # Number of GRPO iterations per update\n",
    "    \n",
    "    # GRPO specific hyperparameters\n",
    "    clip_epsilon: float = 0.2  # PPO clipping parameter\n",
    "    kl_beta: float = 0.02      # KL divergence coefficient\n",
    "\n",
    "    # Training configuration\n",
    "    # gradient_accumulation_steps: int = 1\n",
    "    # warmup_steps: int = 100\n",
    "    # max_grad_norm: float = 1.0\n",
    "    seed: int = 42\n",
    "    \n",
    "    # Dataset configuration\n",
    "    dataset_name: str = \"syllogism\"\n",
    "    dataset_size: int = 1000\n",
    "    \n",
    "    # Device configuration\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Optimization\n",
    "    adam_epsilon: float = 1e-8\n",
    "    weight_decay: float = 0.01\n",
    "    \n",
    "    # Logging and saving\n",
    "    log_interval: int = 10\n",
    "    save_interval: int = 100\n",
    "    eval_interval: int = 50\n",
    "    \n",
    "    # Generation parameters\n",
    "    temperature: float = 1.0\n",
    "    top_p: float = 0.9\n",
    "    top_k: int = 50\n",
    "    do_sample: bool = True\n",
    "# Initialize configuration\n",
    "config = GRPOConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a291d9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRPO Configuration:\n",
      "==================================================\n",
      "model_name               : Qwen/Qwen2.5-0.5B-Instruct\n",
      "learning_rate            : 0.0003\n",
      "batch_size               : 32\n",
      "num_updates              : 1000\n",
      "max_steps                : 20\n",
      "n_outputs                : 4\n",
      "max_length               : 256\n",
      "grpo_iterations          : 4\n",
      "clip_epsilon             : 0.2\n",
      "kl_beta                  : 0.02\n",
      "seed                     : 42\n",
      "dataset_name             : syllogism\n",
      "dataset_size             : 1000\n",
      "device                   : cuda\n",
      "adam_epsilon             : 1e-08\n",
      "weight_decay             : 0.01\n",
      "log_interval             : 10\n",
      "save_interval            : 100\n",
      "eval_interval            : 50\n",
      "temperature              : 1.0\n",
      "top_p                    : 0.9\n",
      "top_k                    : 50\n",
      "do_sample                : True\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Display configuration\n",
    "print(\"GRPO Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "for field, value in config.__dict__.items():\n",
    "    print(f\"{field:<25}: {value}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b948f426",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: Qwen/Qwen2.5-0.5B-Instruct\n",
      "Device: cuda\n",
      "Model parameters: 494,032,768\n"
     ]
    }
   ],
   "source": [
    "# Load a small LLM using configuration\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.model_name)\n",
    "\n",
    "# Add padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Model loaded: {config.model_name}\")\n",
    "print(f\"Device: {config.device}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0da38ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: syllogism\n",
      "Dataset size: 1000\n",
      "Sample data point:\n",
      "Question: Consider these statements:\n",
      "1. No students are humans\n",
      "2. All humans are chefs\n",
      "\n",
      "Does it logically follow that:\n",
      "Some chefs are humans?\n",
      "(Answer Yes or No)\n",
      "Answer: Yes\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset using configuration\n",
    "\n",
    "dataset = reasoning_gym.create_dataset(config.dataset_name, size=config.dataset_size, seed=config.seed)\n",
    "\n",
    "print(f\"Dataset loaded: {config.dataset_name}\")\n",
    "print(f\"Dataset size: {config.dataset_size}\")\n",
    "print(f\"Sample data point:\")\n",
    "for i, data in enumerate(dataset):\n",
    "    print(f\"Question: {data['question']}\")\n",
    "    print(f\"Answer: {data['answer']}\")\n",
    "    if i == 0:  # Show only first sample\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d180f563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Consider these statements:\\n1. No students are humans\\n2. All humans are chefs\\n\\nDoes it logically follow that:\\nSome chefs are humans?\\n(Answer Yes or No)', 'answer': 'Yes', 'metadata': {'source_dataset': 'syllogism', 'source_index': 0, 'premise1': 'No students are humans', 'premise2': 'All humans are chefs', 'selected_premise': 2, 'conclusion': 'Some chefs are humans', 'is_valid': True, 'type': 'inversion'}}\n",
      "Input: Consider these statements:\n",
      "1. No students are humans\n",
      "2. All humans are chefs\n",
      "\n",
      "Does it logically follow that:\n",
      "Some chefs are humans?\n",
      "(Answer Yes or No) Yes\n",
      "Generated: Consider these statements:\n",
      "1. No students are humans\n",
      "2. All humans are chefs\n",
      "\n",
      "Does it logically follow that:\n",
      "Some chefs are humans?\n",
      "(Answer Yes or No) Yes.\n",
      "\n",
      "Let's break this down step-by-step:\n",
      "\n",
      "1. The first\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for data in dataset:\n",
    "    print(data)\n",
    "    # Prepare the input\n",
    "    input_text = data['question'] + \" \" + data['answer']\n",
    "    inputs = tokenizer(input_text, return_tensors='pt')\n",
    "\n",
    "    # Generate output\n",
    "    outputs = model.generate(**inputs, max_length=50)\n",
    "\n",
    "    # Decode the output\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Input: {input_text}\\nGenerated: {generated_text}\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cb4ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRPO:\n",
    "    def __init__(self, model, tokenizer, config: GRPOConfig):\n",
    "        super(GRPO, self).__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "        \n",
    "        # Move model to device\n",
    "        self.model.to(config.device)\n",
    "\n",
    "    def compute_kl_term(self, ref_log_probs, current_model_log_probs):\n",
    "        \"\"\"Compute KL divergence term for regularization\"\"\"\n",
    "        res = (ref_log_probs - current_model_log_probs).exp() - (\n",
    "            torch.log(ref_log_probs.exp()) - torch.log(current_model_log_probs.exp())\n",
    "        ) - 1\n",
    "        return res\n",
    "    \n",
    "    def loss_function(self, old_log_probs, old_model_log_probs, advantages):\n",
    "        \"\"\"GRPO loss function with clipping\"\"\"\n",
    "        logratio = old_log_probs - old_model_log_probs\n",
    "        ratio = logratio.exp()\n",
    "        surrogate1 = ratio * advantages\n",
    "        surrogate2 = torch.clamp(ratio, 1 - self.config.clip_epsilon, 1 + self.config.clip_epsilon) * advantages\n",
    "        loss = -torch.min(surrogate1, surrogate2).mean()\n",
    "        return loss\n",
    "    \n",
    "    def compute_advantages(self, scores, rewards):\n",
    "        \"\"\"Compute advantages using score and rewards\"\"\"\n",
    "        normalized_reward = (scores - rewards.mean()) / (rewards.std() + 1e-8)\n",
    "        return normalized_reward\n",
    "    \n",
    "    def generate_text(self, input_text, max_length=None):\n",
    "        \"\"\"Generate text using the model with config parameters\"\"\"\n",
    "        \n",
    "        prompt = '''\n",
    "        \n",
    "        A conversation between User and Assistant. The User asks a question, and the Assistant provides an answer.\n",
    "        The assistant first thinks about the reasoning process in mind, and then provide the user with the answer. The reasoning process and the answer are to be enclosed within <think> and </think>, <answer> </answer> tags respectively.\n",
    "        i.e, <think> reasoning process here </think> <answer> answer here </answer>. User {}. Assistant:  \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        final_input = prompt.format(input_text)\n",
    "        \n",
    "        max_length = max_length or self.config.max_length\n",
    "        inputs = self.tokenizer(final_input, return_tensors='pt').to(self.config.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs, \n",
    "                padding=True,\n",
    "                max_length=max_length,\n",
    "                temperature=self.config.temperature,\n",
    "                top_p=self.config.top_p,\n",
    "                top_k=self.config.top_k,\n",
    "                do_sample=self.config.do_sample,\n",
    "            )\n",
    "        \n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29cb4ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset type: <class 'reasoning_gym.logic.syllogisms.SyllogismDataset'>\n",
      "Available methods: ['DEFAULT_TERMS', 'category', 'config', 'score_answer', 'seed', 'size', 'terms']\n",
      "Sampled batch of 5 items:\n",
      "Item 1: {'question': 'Consider these statements:\\n1. All horses are tigers\\n2. All tigers are musicians\\n\\nDoes it logically follow that:\\nSome horses are musicians?\\n(Answer Yes or No)', 'answer': 'Yes', 'metadata': {'source_dataset': 'syllogism', 'source_index': 92, 'premise1': 'All horses are tigers', 'premise2': 'All tigers are musicians', 'conclusion': 'Some horses are musicians', 'is_valid': True, 'type': 'syllogism'}}\n",
      "Item 2: {'question': 'Consider these statements:\\n1. Some cats are musicians\\n2. Some musicians are adults\\n\\nDoes it logically follow that:\\nSome musicians are cats?\\n(Answer Yes or No)', 'answer': 'Yes', 'metadata': {'source_dataset': 'syllogism', 'source_index': 934, 'premise1': 'Some cats are musicians', 'premise2': 'Some musicians are adults', 'selected_premise': 1, 'conclusion': 'Some musicians are cats', 'is_valid': True, 'type': 'inversion'}}\n",
      "Item 3: {'question': 'Consider these statements:\\n1. Some chefs are tigers\\n2. Some tigers are not musicians\\n\\nDoes it logically follow that:\\nSome tigers are chefs?\\n(Answer Yes or No)', 'answer': 'Yes', 'metadata': {'source_dataset': 'syllogism', 'source_index': 166, 'premise1': 'Some chefs are tigers', 'premise2': 'Some tigers are not musicians', 'selected_premise': 1, 'conclusion': 'Some tigers are chefs', 'is_valid': True, 'type': 'inversion'}}\n",
      "Item 4: {'question': 'Consider these statements:\\n1. All artists are dogs\\n2. Some dogs are not dolphins\\n\\nDoes it logically follow that:\\nSome artists are not dolphins?\\n(Answer Yes or No)', 'answer': 'Yes', 'metadata': {'source_dataset': 'syllogism', 'source_index': 606, 'premise1': 'All artists are dogs', 'premise2': 'Some dogs are not dolphins', 'conclusion': 'Some artists are not dolphins', 'is_valid': True, 'type': 'syllogism'}}\n",
      "Item 5: {'question': 'Consider these statements:\\n1. All dolphins are teachers\\n2. All teachers are students\\n\\nDoes it logically follow that:\\nAll dolphins are students?\\n(Answer Yes or No)', 'answer': 'Yes', 'metadata': {'source_dataset': 'syllogism', 'source_index': 283, 'premise1': 'All dolphins are teachers', 'premise2': 'All teachers are students', 'conclusion': 'All dolphins are students', 'is_valid': True, 'type': 'syllogism'}}\n"
     ]
    }
   ],
   "source": [
    "#Dataset Creation and Sampling\n",
    "\n",
    "# Check what methods are available on the dataset\n",
    "print(\"Dataset type:\", type(dataset))\n",
    "print(\"Available methods:\", [method for method in dir(dataset) if not method.startswith('_')])\n",
    "\n",
    "# Since procedural datasets don't have .sample(), we need to use random sampling\n",
    "import random\n",
    "\n",
    "def sample_from_dataset(dataset, n):\n",
    "    \"\"\"Sample n items from the dataset\"\"\"\n",
    "    # Convert dataset to list if it's iterable\n",
    "    dataset_list = list(dataset) if hasattr(dataset, '__iter__') else dataset\n",
    "    \n",
    "    # If the dataset is smaller than n, return all items\n",
    "    if len(dataset_list) < n:\n",
    "        return dataset_list\n",
    "    \n",
    "    # Random sample without replacement\n",
    "    return random.sample(dataset_list, n)\n",
    "\n",
    "# Sample 5 data points for demonstration\n",
    "batch = sample_from_dataset(dataset, 5)\n",
    "print(f\"Sampled batch of {len(batch)} items:\")\n",
    "for i, item in enumerate(batch):\n",
    "    print(f\"Item {i+1}: {item}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecedd0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GRPO trainer\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "\n",
    "# Convert dataset to list for sampling\n",
    "dataset_list = list(dataset)\n",
    "print(f\"Total dataset size: {len(dataset_list)}\")\n",
    "\n",
    "grpo = GRPO(model, tokenizer, config)\n",
    "\n",
    "def sample_batch(dataset_list, batch_size):\n",
    "    \"\"\"Sample a batch from the dataset\"\"\"\n",
    "    return random.sample(dataset_list, min(batch_size, len(dataset_list)))\n",
    "\n",
    "def extract_answer_with_regex(text):\n",
    "    \"\"\"Extract text between <answer> and </answer> tags using regex\"\"\"\n",
    "    # Pattern to match text between <answer> and </answer> tags\n",
    "    # .*? makes it non-greedy (stops at first </answer>)\n",
    "    # re.DOTALL flag makes . match newlines too\n",
    "    pattern = r'<answer>(.*?)</answer>'\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return \"\"  # Return empty string if no match found\n",
    "\n",
    "def extract_thinking_with_regex(text):\n",
    "    \"\"\"Extract text between <think> and </think> tags using regex\"\"\"\n",
    "    pattern = r'<think>(.*?)</think>'\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "for i in tqdm(range(config.num_updates), desc=\"Training\"):\n",
    "    # Sample a batch of data using our custom sampling function\n",
    "    \n",
    "    for step in range(config.max_steps):\n",
    "        # Sample a batch of data using our custom sampling function\n",
    "        batch = sample_batch(dataset_list, config.batch_size)\n",
    "        outputs = []\n",
    "        rewards = []\n",
    "        for data in batch:\n",
    "            \n",
    "            #Compute n outputs per datapoint\n",
    "            for _ in range(config.n_outputs):\n",
    "                output = grpo.generate_text(data['question'], max_length=config.max_length)\n",
    "                outputs.append((output, data['answer']))\n",
    "\n",
    "            for output, answer in outputs:\n",
    "                # Use regex to extract answer instead of string splitting\n",
    "                extracted_answer = extract_answer_with_regex(output)\n",
    "                extracted_thinking = extract_thinking_with_regex(output)\n",
    "                \n",
    "                score = dataset.score_answer(extracted_answer, answer) == 1.0\n",
    "                rewards.append(score)\n",
    "                \n",
    "            advantages = grpo.compute_advantages(torch.tensor(rewards), torch.tensor(rewards))\n",
    "            \n",
    "            for _ in range(config.grpo_iterations):\n",
    "                old_log_probs = \n",
    "                loss = grpo.loss_function()\n",
    "                # You can now use extracted_answer and extracted_thinking\n",
    "                # For example, compute rewards based on extracted_answer\n",
    "                # reward = compute_reward(extracted_answer, data['answer'])\n",
    "                # rewards.append(reward)\n",
    "             \n",
    "    # Backward pass and optimization\n",
    "    # Note: You need to define 'loss' before this point\n",
    "    # optimizer.zero_grad()\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "    \n",
    "    if i % config.log_interval == 0:\n",
    "        print(f\"Update {i}\")\n",
    "        # print(f\"Update {i}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "print(\"Training completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
